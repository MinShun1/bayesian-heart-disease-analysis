---
title: "Bayesian Logistic Regression for Heart Disease (Hypertuned Version)"
output: html_document
date: "2024-11-30"
---

## Overview
The analysis investigates the **Heart Disease Dataset** using **Bayesian Logistic Regression**, where the target variable (`num`) is **binarized** into two categories:  

- `0` → No heart disease (stage 0)  
- `1` → Presence of heart disease (stage 1 & 2)  

All predictors are standardized prior to estimation. Two modeling strategies are compared:
1. **Baseline model** with uninformative priors  
2. **Hypertuned model** with informative priors derived from domain knowledge and prior studies  

Model performance is assessed through **convergence diagnostics** (trace plots, Gelman–Rubin statistics, Geweke tests) and **model comparison metrics** (WAIC, DIC, posterior predictive checks). The hypertuned model demonstrates superior convergence and out-of-sample fit, and is therefore selected for further analysis.  

Subsequent sections extend this best-performing model in two directions:  
- **Predictive Analysis**: characterizes uncertainty in patient-level and population-level outcomes using posterior sampling.  
- **Prescriptive Analysis**: incorporates explicit cost structures to derive treatment policies that balance the risks of false negatives and false positives.  

This two-stage approach enables both uncertainty-aware predictions and cost-sensitive decision rules to be evaluated under a coherent Bayesian framework.  

```{r}
library(rjags)

# Load dataset
df <- read.csv("C:/Users/Min Shun/Desktop/BINUS/SMT 3/Bayesian Data Analysis/Bayesian_Mini_Project/heart_disease_uci_clean.csv")

# Binarize target variable:
#   - 0 if stage = 0
#   - 1 if stage = 1 or 2
Y <- ifelse(df$num == 0, 0, 1)

# Scale predictors
X <- as.matrix(df[, 3:11])

# Data dimensions and MCMC setup
n <- length(Y)
burn <- 1000
iters <- 5000
chains <- 2
```

### Preview of the dataset
```{r}
# Show the first rows of the dataset
head(df)

# Summary statistics
summary(df)
```

### JAGS Modeling
```{r}
mod <- textConnection("model{
  for(i in 1:n){
    Y[i] ~ dbern(pi[i])                              # Binary outcome
    logit(pi[i]) <- beta[1] + inprod(X[i, ], beta[2:10])  
  }

  # Informative priors (based on field observation)
  beta[1] ~ dnorm(0, 0.01)                           # Intercept
  beta[2] ~ dnorm(0.5402041, 1/(9.09^2))             # 'age'
  beta[3] ~ dnorm(0.69, 1/(0.48^2))                  # 'sex'
  beta[4] ~ dnorm(0.6581, 1/(17.55^2))               # 'trestbps'
  beta[5] ~ dnorm(0.4096186, 1/(52.77^2))            # 'chol'
  beta[6] ~ dnorm(0.16, 1/(0.37^2))                  # 'fbs'
  beta[7] ~ dnorm(0.6266901, 1/(22.57^2))            # 'thalch'
  beta[8] ~ dnorm(0.38, 1/(0.48^2))                  # 'exang'
  beta[9] ~ dnorm(0.4147727, 1/(1.17^2))             # 'oldpeak'
  beta[10] ~ dnorm(0.75, 1/(0.63^2))                 # 'slope'
}")
```

### Run Model
```{r}
# Provide data for JAGS
data <- list(Y = Y, X = X, n = n)

# Initialize the model
model <- jags.model(mod, data = data, n.chains = chains, quiet = TRUE)

# Burn-in phase
update(model, burn, progress.bar = "none")

# Sample from the posterior
samps <- coda.samples(model, variable.names = c("beta"), 
                      n.iter = iters, n.thin = 5, progress.bar = "none")

post <- as.matrix(samps)
```

### Predictive Analysis
This section applies Bayesian predictive modeling with Thompson Sampling to estimate the probability of treatment decisions under uncertainty in the posterior distribution of model parameters.

The predictive approach answers the question:

“Based on the data and posterior uncertainty, what is the probability distribution of outcomes for a given patient (or group of patients)?”

It does not explicitly minimize costs or prescribe optimal actions (like the prescriptive policies), but rather focuses on quantifying uncertainty in predictions.

The illustration is provided at two levels:

### Predictive (Patient-Level, Thompson Sampling with 0.5 threshold)
For specific patients (e.g., id = 10 and id = 11), repeated sampling from the posterior provides a distribution of treatment probabilities under a simple rule (treat if p > 0.5). This highlights the uncertainty in predictions for individual patients.
```{r}
set.seed(123)
N_iter <- 100   # Number of Thompson Sampling iterations

# Patient with id = 10
row_10 <- which(df$id == 10)
new_x_10 <- as.numeric(X[row_10, ])

actions_10 <- numeric(N_iter)
probs_10 <- numeric(N_iter)

for (i in 1:N_iter) {
  beta_draw <- post[sample(1:nrow(post), 1), ]
  logit_p <- beta_draw[1] + sum(new_x_10 * beta_draw[2:10])
  p <- 1 / (1 + exp(-logit_p))
  
  probs_10[i] <- p
  actions_10[i] <- ifelse(p > 0.5, 1, 0)
}

cat("Proportion of times treatment was chosen for patient id=10:", mean(actions_10), "\n")
cat("Ground truth (actual outcome) for patient id=10:", Y[row_10], "\n")
summary(probs_10)
hist(probs_10, main="Posterior Predictive Probabilities (id=10)", col="skyblue")

# Patient with id = 11
row_11 <- which(df$id == 11)
new_x_11 <- as.numeric(X[row_11, ])

actions_11 <- numeric(N_iter)
probs_11 <- numeric(N_iter)

for (i in 1:N_iter) {
  beta_draw <- post[sample(1:nrow(post), 1), ]
  logit_p <- beta_draw[1] + sum(new_x_11 * beta_draw[2:10])
  p <- 1 / (1 + exp(-logit_p))
  
  probs_11[i] <- p
  actions_11[i] <- ifelse(p > 0.5, 1, 0)
}

cat("Proportion of times treatment was chosen for patient id=11:", mean(actions_11), "\n")
cat("Ground truth (actual outcome) for patient id=11:", Y[row_11], "\n")
summary(probs_11)
hist(probs_11, main="Posterior Predictive Probabilities (id=11)", col="orange")
```

### Predictive (Population-Level, Thompson Sampling with 0.5 threshold)
Extending the same approach to all patients, predictive accuracy is evaluated across multiple posterior draws, and the average treatment frequency per patient is reported.
```{r}
n <- nrow(X)
all_preds <- matrix(0, nrow=N_iter, ncol=n)

for (i in 1:N_iter) {
  # Sample beta from posterior
  beta_draw <- post[sample(1:nrow(post), 1), ]
  
  # Predict probability for all patients
  logit_p <- beta_draw[1] + as.matrix(X) %*% beta_draw[2:10]
  probs <- 1 / (1 + exp(-logit_p))
  
  # Decision rule: treat if p > 0.5
  all_preds[i, ] <- ifelse(probs > 0.5, 1, 0)
}

# Accuracy across iterations
accs <- apply(all_preds, 1, function(pred) mean(pred == Y))
cat("Average accuracy (Thompson Sampling, 0.5 threshold):", mean(accs), "\n")
summary(accs)

# Proportion of times each patient received treatment
treatment_freq_naive <- colMeans(all_preds)
summary(treatment_freq_naive)
```

### Prescriptive Analysis
This section move beyond descriptive and predictive modeling to prescriptive decision-making. The goal is to determine optimal treatment policies for patients, considering not only predicted probabilities but also the cost of decision errors.

Case context: In a clinical setting, a false negative (FN) (predicting a patient as healthy when they actually have a disease) can be very costly, potentially leading to severe health consequences. On the other hand, a false positive (FP) (predicting disease when the patient is healthy) is usually less costly, as it may only result in additional tests or minor overtreatment.

Therefore, in our cost structure, FN is penalized more heavily than FP.

This section explore two policies for prescribing treatment decisions:

### Analytic Policy (Cost-Aware Thresholding)
A cost-aware analytic threshold is derived from the ratio of false positive and false negative costs (1:10). This threshold ensures that treatment is assigned only when the posterior probability of disease is high enough to justify the risk of overtreatment.
```{r}
# cost_FP = cost of false positive (predict disease when patient is healthy)
# cost_FN = cost of false negative (predict healthy when patient has disease)

cost_FP <- 1
cost_FN <- 10   # assign higher penalty to false negatives
# “missing a sick patient is about ten times worse than overtreating a healthy patient”.

# Optimal threshold = cost_FP / (cost_FP + cost_FN)
threshold <- cost_FP / (cost_FP + cost_FN)

# Posterior mean coefficients
beta_hat <- colMeans(post)

# Posterior predictive mean probability
logit_p_all <- beta_hat[1] + as.matrix(X) %*% beta_hat[2:10]
p_mean <- 1 / (1 + exp(-logit_p_all))

# Apply decision rule
pred_analytic <- ifelse(p_mean > threshold, 1, 0)

# Evaluate accuracy and total expected cost
table(Predicted = pred_analytic, Actual = Y)
FP <- sum(pred_analytic == 1 & Y == 0)
FN <- sum(pred_analytic == 0 & Y == 1)
total_cost_analytic <- cost_FP * FP + cost_FN * FN
cat("Total cost (Analytic policy):", total_cost_analytic, "\n")
```

### Thompson Sampling Policy (Cost-Aware)
Instead of relying only on posterior mean probabilities, posterior sampling is used to capture uncertainty in the model. For each draw from the posterior, the expected cost of treating versus not treating each patient is computed, and the action with the minimum expected cost is chosen. This produces a distribution of treatment decisions, from which the final policy is derived.
```{r}
all_actions <- matrix(0, nrow=N_iter, ncol=n)

posterior_n <- nrow(post)
sample_idx <- sample(1:posterior_n, size=N_iter, replace=TRUE)

for (i in 1:N_iter) {
  beta_draw <- post[sample_idx[i], ]
  logit_p <- beta_draw[1] + as.matrix(X) %*% beta_draw[2:10]
  p_draw <- 1 / (1 + exp(-logit_p))
  
  # Expected costs of each action per patient
  # If treat: risk is overtreatment (false positive) → cost = (1 - p) * cost_FP
  # If no treat: risk is missed disease (false negative) → cost = p * cost_FN
  cost_if_treat <- (1 - p_draw) * cost_FP
  cost_if_no_treat <- p_draw * cost_FN
  
  # Choose the action that minimizes expected cost
  actions <- ifelse(cost_if_treat <= cost_if_no_treat, 1, 0)
  all_actions[i, ] <- actions
}

# Aggregate: treatment frequency across draws
treatment_freq_ts <- colMeans(all_actions)

# Final policy: treat if treatment_freq > 0.5
policy_preds <- ifelse(treatment_freq_ts > 0.5, 1, 0)

# Evaluate cost of this policy
table(Predicted = policy_preds, Actual = Y)
FP_ts <- sum(policy_preds == 1 & Y == 0)
FN_ts <- sum(policy_preds == 0 & Y == 1)
total_cost_ts <- cost_FP * FP_ts + cost_FN * FN_ts
cat("Total cost (Thompson Sampling policy):", total_cost_ts, "\n")

# Summary of treatment frequencies
summary(treatment_freq_ts)
```